---
title: "Lab 4c. Deep Learning - iNaturalist"
editor_options: 
  chunk_output_type: inline
---
# Set Up 

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
librarian::shelf(
  tidyverse,
  tensorflow, 
  digest,
  keras,
  digest, 
  dplyr, 
  DT, 
  glue, 
  keras, 
  purrr, 
  readr, 
  stringr, 
  tidyr
)

# install Python into user space
(reticulate::miniconda_path()) # show the Python path
if (!file.exists(reticulate::miniconda_path()))
  reticulate::install_miniconda()

# install keras with tensorflow
if (!keras::is_keras_available())
  keras::install_keras()
```

# Data Pre-Processing 

```{r}
# path to folder containing species directories of images
dir_train_mini <- "/courses/EDS232/inaturalist-2021/train_mini"

# get list of directories, one per species (n = 10,000 species)
dirs_spp <- list.dirs(dir_train_mini, recursive = F)
n_spp <- length(dirs_spp)
n_spp
```


```{r}
# set seed (for reproducible results) 
# just before sampling (otherwise get different results)
# based on your username (unique amongst class)
Sys.info()[["user"]] %>% 
  digest::digest2int() %>% 
  set.seed()
i10 <- sample(1:n_spp, 10)

# show the 10 indices sampled of the 10,000 possible 
i10
sp10 <- basename(dirs_spp)[i10]
```


```{r}
# show the 10 species directory names 
basename(dirs_spp)[i10] 
```


```{r}
# show the first 2 species directory names
i2 <- i10[1:2]
sp2 <- basename(dirs_spp)[i2]

```

```{r}
# path to output table of paths, which could be read by R, eg readr::read_csv(), or Python, eg pandas.read_csv()
inat_spp_images_csv <- "~/inat_spp_images.csv"

d <- tibble(
  # get 10 species names
  species = basename(dirs_spp)[i10],
  # assign TRUE/FALSE for: 10 species (multi-class) and 2 species (binary)
  spp10 = TRUE,
  spp2  = c(T,T,rep(F,8)))
DT::datatable(d)
```

```{r}
d <- d %>% 
  mutate(
    # construct full path to species directory
    dir_species = file.path(dir_train_mini, species),
    tbl_images  = purrr::map(dir_species, function(dir){
      # create a tibble per species
      tibble(
        # list files in the species directory (n=50)
        image = list.files(dir),
        # assign subset per species
        subset = c(rep("train", 30), rep("validation", 10), rep("test", 10))) })) %>% 
  # go from a tibble with 10 species rows containing a nested tbl_images to unnested, ie 10 species * 50 images = 500 rows
  tidyr::unnest(tbl_images)

# write tibble to CSV file for subsequent reading
## save CSV in data folder 
readr::write_csv(d, "data/inat_spp_images.csv", inat_spp_images_csv)

# show counts of image files per species and subset
d %>% 
  mutate(
    # truncate species to show one line per species
    species_trunc = stringr::str_trunc(species, 40)) %>% 
  select(species_trunc, subset) %>% 
  table()
```

# Create training, validation, and test directories 
```{r}
original_dataset_dir <- "/courses/EDS232/inaturalist-2021/train_mini"
base_dir             <- "~/eds232/eds232-dl/data"
```


# 1. **2 Species (binary classification) - neural net**. 

- Based on the 3.4 Movies example (binary classification)
- I will be using 2 randomly assigned species: 
  - `r obtusa`
  - `r chinensis`
  
  ## Create general directories for binary classification 
```{r}
# base directories 
train_dir_2 <- file.path(base_dir, "train_2")
validation_dir_2 <- file.path(base_dir, "validation_2")
test_dir_2 <- file.path(base_dir, "test_2")

# create base train, validate, test directories 
dir.create(train_dir_2)
dir.create(validation_dir_2)
dir.create(test_dir_2)
```
  
- I will subset the iNaturalist data into 3 groups: 
  1. train: n = 30 per species (n = 60 in total for 2 species)
  2. validate: n = 10 per species (n = 20 in total for 2 species)
  3. test: n = 10 per species (n = 20 in total for 2 species)

## Create directories for binary classification of 2 species 
```{r}
# for loop to create species specific train, validate, and test directories
# named directories using the unique 5 digit characters at the start of the file
for (i in 1:length(sp2)){
  dir.create(file.path(train_dir_2, str_sub(sp2[[i]], start = 1, end = 5)))
  dir.create(file.path(validation_dir_2, str_sub(sp2[[i]], start = 1, end = 5)))
  dir.create(file.path(test_dir_2, str_sub(sp2[[i]], start = 1, end = 5)))
}
```

## Add images into the directories for binary classification of 2 species 

```{r}
# create test, validation, and training groups of images
for(i in 1:length(sp2)){
  # create 5 groups of 10 random samples
  species_samples_2 <- replicate(5, sample(list.files(paste0(original_dataset_dir, "/", sp2[[i]]), 
                                                    full.names = TRUE), replace = FALSE, 10))
  ## train n = 30 ##
  train <- rbind(species_samples_2[,1], species_samples_2[,2], species_samples_2[,3])
  file.copy(from = train, 
            to = paste0(train_dir_2, "/", str_sub(sp2[[i]], start = 1, end = 5)), overwrite = TRUE)
  ## validation n = 10 ##
  validate <- species_samples_2[,4]
  file.copy(from = validate,
            to = paste0(validation_dir_2, "/", str_sub(sp2[[i]], start = 1, end = 5)), overwrite = TRUE)
  ## train n = 10 ##
  test <- species_samples_2[,5]
  file.copy(from = test,
            to = paste0(test_dir_2, "/", str_sub(sp2[[i]], start = 1, end = 5)), overwrite = TRUE)
}
```


```{r}
# sanity check that the training dataset has 30, the validation set has 10, and the test dataset has 10 images per species 
for (i in 1:length(sp2)){
  cat("total training images", length(list.files(file.path(train_dir_2, str_sub(sp2[[i]], start = 1, end = 5)))), "\n")
  cat("total validation images", length(list.files(file.path(validation_dir_2, str_sub(sp2[[i]], start = 1, end = 5)))), "\n")
  cat("total test images", length(list.files(file.path(test_dir_2, str_sub(sp2[[i]], start = 1, end = 5)))), "\n")
}
```

## Rescale images 
```{r}
# All images will be rescaled by 1/255
train_datagen <- image_data_generator(rescale = 1/255)
validation_datagen <- image_data_generator(rescale = 1/255)
test_datagen <- image_data_generator(rescale = 1/255)

train_generator <- flow_images_from_directory(
  # This is the target directory
  train_dir_2,
  # This is the data generator
  train_datagen,
  # All images will be resized to 150x150
  target_size = c(150, 150),
  batch_size = 5,
  # Since we use binary_crossentropy loss, we need binary labels
  class_mode = "binary")

validation_generator <- flow_images_from_directory(
  validation_dir_2,
  validation_datagen,
  target_size = c(150, 150),
  batch_size = 5,
  class_mode = "binary")

test_generator <- flow_images_from_directory(
  test_dir_2,
  test_datagen,
  target_size = c(150, 150),
  batch_size = 5,
  class_mode = "binary")
```

```{r}
batch <- generator_next(train_generator)
str(batch)
```

## Build the network 
```{r}
model <- keras_model_sequential() %>% 
  layer_dense(units = 16, activation = "relu", input_shape = c(150, 150, 3)) %>%
  layer_flatten() %>% 
  layer_dense(units = 16, activation = "relu") %>% 
  layer_dense(units =  1, activation = "sigmoid")
```

```{r}
model %>% compile(
  optimizer = "rmsprop",
  loss      = "binary_crossentropy",
  metrics   = c("accuracy"))
```


```{r}
history <- model %>% fit(
    train_generator,
    steps_per_epoch = 5,
    epochs = 30,
    validation_data = validation_generator,
    validation_steps = 5)
```

```{r}
plot(history)
```
## Evaluate 

```{r}
test_generator <- flow_images_from_directory(
  test_dir_2,
  test_datagen,
  target_size = c(150, 150),
  batch_size = 20,
  class_mode = "binary"
)
model %>% evaluate_generator(test_generator, steps = 50)
```


# **2 Species (binary classification) - convolutional neural net**. 

## Make the new model 
```{r}
# make the new model  
model <- keras_model_sequential() %>% 
  layer_conv_2d(
    filters = 32, kernel_size = c(3, 3), activation = "relu",
    input_shape = c(150, 150, 3)) %>% 
  layer_max_pooling_2d(pool_size = c(2, 2)) %>% 
  layer_conv_2d(filters = 64, kernel_size = c(3, 3), activation = "relu") %>% 
  layer_max_pooling_2d(pool_size = c(2, 2)) %>% 
  layer_conv_2d(filters = 128, kernel_size = c(3, 3), activation = "relu") %>% 
  layer_max_pooling_2d(pool_size = c(2, 2)) %>% 
  layer_conv_2d(filters = 128, kernel_size = c(3, 3), activation = "relu") %>% 
  layer_max_pooling_2d(pool_size = c(2, 2)) %>% 
  layer_flatten() %>% 
  layer_dropout(rate = 0.5) %>% 
  layer_dense(units = 512, activation = "relu") %>% 
  layer_dense(units = 1, activation = "sigmoid")
```

## Compile the model 
```{r}
model %>% compile(
  loss = "binary_crossentropy",
  optimizer = optimizer_rmsprop(learning_rate = 1e-4),
  metrics = c("acc"))
```

## Fit the model 
```{r}
history_2 <- model %>% fit(
    train_generator,
    steps_per_epoch = 5,
    epochs = 30,
    validation_data = validation_generator,
    validation_steps = 5)
```

```{r}
history_2
```

## Evaluate 
```{r}
test_generator_2 <- flow_images_from_directory(
  test_dir_2,
  test_datagen,
  target_size = c(150, 150),
  batch_size = 5,
  class_mode = "binary"
)
model %>% evaluate(test_generator_2, steps = 50)
#maybe change steps to a smaller number ?? 
```

# Create directories for multi-class classifications (10 species)

```{r}
original_dataset_dir <- "/courses/EDS232/inaturalist-2021/train_mini"
base_dir             <- "~/eds232/eds232-dl/data"

# base directories 
train_dir_10 <- file.path(base_dir, "train_10")
validation_dir_10 <- file.path(base_dir, "validation_10")
test_dir_10 <- file.path(base_dir, "test_10")

# create base train, validate, test directories 
dir.create(train_dir_10)
dir.create(validation_dir_10)
dir.create(test_dir_10)

for (i in 1:length(sp10)){
  dir.create(file.path(train_dir_10, str_sub(sp10[[i]], start = 1, end = 5)))
  dir.create(file.path(validation_dir_10, str_sub(sp10[[i]], start = 1, end = 5)))
  dir.create(file.path(test_dir_10, str_sub(sp10[[i]], start = 1, end = 5)))
}
```

## Add images into directories for 10 species 
```{r}
for(i in 1:length(sp10)){
  # create 5 groups of 10 random samples
  species_samples_10 <- replicate(5, 
                                  sample(list.files(paste0(original_dataset_dir, "/", sp10[[i]]), 
                                                    full.names = TRUE), replace = FALSE, 10))
  ## train n = 30 ##
  train <- rbind(species_samples_10[,1], species_samples_10[,2], species_samples_10[,3])
  file.copy(from = train, 
            to = paste0(train_dir_10, "/", str_sub(sp10[[i]], start = 1, end = 5)),
            overwrite = TRUE)
  ## validation n = 10 ##
  validate <- species_samples_10[,4]
  file.copy(from = validate,
            to = paste0(validation_dir_10, "/", str_sub(sp10[[i]], start = 1, end = 5)),
            overwrite = TRUE)
  ## train n = 10 ##
  test <- species_samples_10[,5]
  file.copy(from = test,
            to = paste0(test_dir_10, "/", str_sub(sp10[[i]], start = 1, end = 5)),
            overwrite = TRUE)
}
```

## Check number of images per folder 
```{r}
# sanity check that the training dataset has 30, the validation set has 10, and the test dataset has 10 images per species 
for (i in 1:length(sp10)){
  cat("total training images", length(list.files(file.path(train_dir_10, str_sub(sp10[[i]], start = 1, end = 5)))), "\n")
  cat("total validation images", length(list.files(file.path(validation_dir_10, str_sub(sp10[[i]], start = 1, end = 5)))), "\n")
  cat("total test images", length(list.files(file.path(test_dir_10, str_sub(sp10[[i]], start = 1, end = 5)))), "\n")
}
```


# 3. **10 Species (multi-class classification) - neural net**

## Pre-process images from 10 species 
```{r}
# pre process the images from the 10 species using categorical class 
# All images will be rescaled by 1/255
test_datagen_10 <- image_data_generator(rescale = 1/255)
train_datagen_10 <- image_data_generator(rescale = 1/255)
validation_datagen_10 <- image_data_generator(rescale = 1/255)

train_generator_10 <- flow_images_from_directory(
  # This is the target directory
  train_dir_10,
  # This is the data generator
  train_datagen_10,
  # All images will be resized to 150x150
  target_size = c(150, 150),
  batch_size = 5,
  # change label to categorical 
  class_mode = "categorical") 

validation_generator_10 <- flow_images_from_directory(
  validation_dir_10,
  validation_datagen_10,
  target_size = c(150, 150),
  batch_size = 5,
  class_mode = "categorical")

test_generator_10 <- flow_images_from_directory(
  test_dir_10,
  test_datagen_10,
  target_size = c(150, 150),
  batch_size = 5,
  class_mode = "categorical")


batch <- generator_next(train_generator_10)
str(batch)
```

## Build the network 

```{r}
model <- keras_model_sequential() %>% 
  layer_dense(units = 16, activation = "relu", input_shape = c(150, 150, 3)) %>%
  layer_flatten() %>% 
  layer_dense(units = 16, activation = "relu") %>% 
  layer_dense(units =  1, activation = "softmax")
```

## Compile 

```{r}
model %>% compile(
  optimizer = "rmsprop",
  loss = "categorical_crossentropy",
  metrics = c("accuracy")
)
```

## Fit the model 

```{r}
history_3 <- model %>% fit(
  train_generator_10,
  steps_per_epoch = 5,
  epochs = 30,
  validation_data = validation_generator_10,
  validation_steps = 5)
```

## Evaluate the model 
```{r}
history_3
```

```{r}
test_generator_3 <- flow_images_from_directory(
  test_dir_10,
  test_datagen_10,
  target_size = c(150, 150),
  batch_size = 20,
  class_mode = "categorical"
)
model %>% evaluate(test_generator_3, steps = 50)
```


# **10 Species (multi-class classification) - convolutional neural net**. 

## Build the network 
```{r}
# make the new model  
model <- keras_model_sequential() %>% 
  layer_conv_2d(
    filters = 32, kernel_size = c(3, 3), activation = "relu",
    input_shape = c(150, 150, 3)) %>% 
  layer_max_pooling_2d(pool_size = c(2, 2)) %>% 
  layer_conv_2d(filters = 64, kernel_size = c(3, 3), activation = "relu") %>% 
  layer_max_pooling_2d(pool_size = c(2, 2)) %>% 
  layer_conv_2d(filters = 128, kernel_size = c(3, 3), activation = "relu") %>% 
  layer_max_pooling_2d(pool_size = c(2, 2)) %>% 
  layer_conv_2d(filters = 128, kernel_size = c(3, 3), activation = "relu") %>% 
  layer_max_pooling_2d(pool_size = c(2, 2)) %>% 
  layer_flatten() %>% 
  layer_dropout(rate = 0.5) %>% 
  layer_dense(units = 512, activation = "relu") %>% 
  layer_dense(units = 1, activation = "sigmoid")  
  
model %>% compile(
  loss = "categorical_crossentropy",
  optimizer = optimizer_rmsprop(learning_rate = 1e-4),
  metrics = c("acc"))
```

## Fit the model 

```{r}
history_3 <- model %>% fit(
    train_generator_10,
    steps_per_epoch = 5,
    epochs = 30,
    validation_data = validation_generator_10,
    validation_steps = 5)
```

## Evaluate the model 

```{r}
history_3
```


```{r}
test_generator_4 <- flow_images_from_directory(
  test_dir_10,
  test_datagen_10,
  target_size = c(150, 150),
  batch_size = 20,
  class_mode = "categorical"
)
model %>% evaluate(test_generator_4, steps = 50)
```










In your models, be sure to include the following:

- Split the original images per species (n=50) into train (n=30), validate (n=10) and test (n=10). These are almost absurdly few files to feed into these complex deep learning models but will serve as a good learning example.

- Include accuracy metric and validation in the fitting process and history plot.

- Evaluate loss and accuracy on your test model results. Compare standard neural network and convolutional neural network results.
